---
title: "Data Cleaning"

---
It is always amazing to see how insights can be generated by data scientists. However, in real life, not all the collected data set can be directly analyzed by the estabilsh tools mainly because of its chaotic nature. 80% of data scientists' time is actually spent on cleaning of the data. So it is important to understand how to handle the messy data and transfer it into organized data ready for further processing.

In this project, we will be introducing some basic techniques that can be applied to the majority of dataset to change super messy data into messy data. It is highly likely that after applying all these techniques, your dataset is still not fully ready because each dataset has its unique problems. And you need to adjust it based on problems. However, know these techniques will definitely help you broaden your way of thinking so 

All of the graphs and code can be found in the MBAN GitHub repository, feel free to download it and modify the numbers for your use case. The Jupyter notebook can be opened in the syzygy.

<a href="https://pims.syzygy.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2FMaster-of-Business-Analytics%2FProject_07_Data_Cleaning&urlpath=tree%2FProject_07_Data_Cleaning%2F" target="_blank" class="button">Launch Syzygy</a>


## Business Problem

Why do we really need to clean the data before applying the statistical analysis to generate business insights? In order to answer this question, we need to know there is a gap between raw data and the insights which managers want to get. Let's use the Retail industry as an example. Some questions that are usually asked in the industry are:
 * Which geographic location provides the most profits per person?
 * What type of customers generate the revenue for us?

However, the raw data set can only record each transaction for customers who enrolled within the loyalty program. And in the process of recording, we can possible miss some of the information required to solve the problem:
 * The transaction only record the name of the store but you have no idea where that store is.
 * The customers are not willing to share gender and age due to privacy issue.

In such scenario, if we directly apply the statistical methods to the raw data set, we might get weird result:
 * The stores in happyland(possibly a community with only 1000 residents)provides the most profits per person.
 * The customers who are not willing to share their gender generate the most revenue.
 
Those insights are only "right" statistically. In business context, they provides no value because we can't have more stores in a community with only 1000 residents, and knowing customer who care about their privacy generate the most revenue doesn't provide a good descriptive term to know who they truly are. So data cleanning is really important in closing the gap.

Let's try to understand how to do it with an example. Here we have a dataset which describes the constuction happens around neighborhoods, can you tell me which neighborhood has the most type 3 construction project?

### Data Set Check


```python
import pandas as pd
import numpy as np
import warnings

# This step prevent warning from showing up for formatting purpose,but ignore this line while practicing yourself
warnings.filterwarnings("ignore")

mydata = pd.read_csv("data/Building_Permits.csv")
```


```python
mydata.head(1)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Permit Number</th>
      <th>Permit Type</th>
      <th>Permit Type Definition</th>
      <th>Permit Creation Date</th>
      <th>Block</th>
      <th>Lot</th>
      <th>Street Number</th>
      <th>Street Number Suffix</th>
      <th>Street Name</th>
      <th>Street Suffix</th>
      <th>...</th>
      <th>Existing Construction Type</th>
      <th>Existing Construction Type Description</th>
      <th>Proposed Construction Type</th>
      <th>Proposed Construction Type Description</th>
      <th>Site Permit</th>
      <th>Supervisor District</th>
      <th>Neighborhoods - Analysis Boundaries</th>
      <th>Zipcode</th>
      <th>Location</th>
      <th>Record ID</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201505065519</td>
      <td>4</td>
      <td>sign - erect</td>
      <td>05/06/2015</td>
      <td>0326</td>
      <td>023</td>
      <td>140</td>
      <td>NaN</td>
      <td>Ellis</td>
      <td>St</td>
      <td>...</td>
      <td>3.0</td>
      <td>constr type 3</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>Tenderloin</td>
      <td>94102.0</td>
      <td>(37.785719256680785, -122.40852313194863)</td>
      <td>1380611233945</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 43 columns</p>
</div>



Before moving into data cleaning, we need to understand what is happening in the dataset so that we can know in which way the data set is "sick:. It is obvious that there are lots of NaN values existing in the dataset. We definitely don't want NA values here otherwise we will be having "security concerning customer" issue. That extra space and and NA value would be what we want to look at first.


```python
# get info about all columns
mydata.info()

# create some exploratory visualization
mydata.isna().sum().plot(kind='bar')
```
![](images/p07_01.png)


## Data Cleaning

### Get Rid Of Extras Spaces

---


```python
string = "   hello world     "

print(string)
print("="*100)

fresh_string = string.strip()
print(fresh_string)
```
```
       hello world     
    ====================================================================================================
    hello world

```
The simple example here show that when there are unneeded spaces before the string and after the string, we want to make sure we can get rid of them.

Then we can apply the same methodology to the entire data set. The major reason why we want to do this is because the extra spaces will lead to both analytical and formatting issue. It is not nice to have extra sapces in front of strings. More importantly, while doing logistic regression analysis," hello" and "hello" in system means two different factors while they are the same in real life.  


### Replacing All NA Values

---

The second step is the most obvious step that we should do. Which is to replace the NA values existing within the dataset with some value that will not influence the analysitcal result. However, there are several techniques that can be applied here:

The first method is fast and easy. We can simply substitute all of them to 0 if we want to get the sum of a column(because 0 does not influence the value of the sum).


```python
test_row = mydata.iloc[0:5]
test_row.fillna(0, inplace=True)
test_row.head(1)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Permit Number</th>
      <th>Permit Type</th>
      <th>Permit Type Definition</th>
      <th>Permit Creation Date</th>
      <th>Block</th>
      <th>Lot</th>
      <th>Street Number</th>
      <th>Street Number Suffix</th>
      <th>Street Name</th>
      <th>Street Suffix</th>
      <th>...</th>
      <th>Existing Construction Type</th>
      <th>Existing Construction Type Description</th>
      <th>Proposed Construction Type</th>
      <th>Proposed Construction Type Description</th>
      <th>Site Permit</th>
      <th>Supervisor District</th>
      <th>Neighborhoods - Analysis Boundaries</th>
      <th>Zipcode</th>
      <th>Location</th>
      <th>Record ID</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201505065519</td>
      <td>4</td>
      <td>sign - erect</td>
      <td>05/06/2015</td>
      <td>0326</td>
      <td>023</td>
      <td>140</td>
      <td>0</td>
      <td>Ellis</td>
      <td>St</td>
      <td>...</td>
      <td>3.0</td>
      <td>constr type 3</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>3.0</td>
      <td>Tenderloin</td>
      <td>94102.0</td>
      <td>(37.785719256680785, -122.40852313194863)</td>
      <td>1380611233945</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 43 columns</p>
</div>



But this methodology is definitely wrong since the proposed construction type description can not be 0. It should be a string instead of number. So how do we want to handle this scenarios since this is a column of strings?

There is simple methodology that can also be applied. We substitute it with the majority of the column. This is particularly useful when we are searching for outliers within the dataframe.


```python
test_row = mydata.iloc[0:100]

test_row['Proposed Construction Type Description'].fillna(test_row['Proposed Construction Type Description'].value_counts()[:1].index.tolist()[0],inplace=True)
#using fillna command while inplace = true to replace the value
#test_row['Proposed Construction Type Description'].value_counts()[:1].index.tolist()[0] returns the most frequent value appear with in the column
test_row.head(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Permit Number</th>
      <th>Permit Type</th>
      <th>Permit Type Definition</th>
      <th>Permit Creation Date</th>
      <th>Block</th>
      <th>Lot</th>
      <th>Street Number</th>
      <th>Street Number Suffix</th>
      <th>Street Name</th>
      <th>Street Suffix</th>
      <th>...</th>
      <th>Existing Construction Type</th>
      <th>Existing Construction Type Description</th>
      <th>Proposed Construction Type</th>
      <th>Proposed Construction Type Description</th>
      <th>Site Permit</th>
      <th>Supervisor District</th>
      <th>Neighborhoods - Analysis Boundaries</th>
      <th>Zipcode</th>
      <th>Location</th>
      <th>Record ID</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201505065519</td>
      <td>4</td>
      <td>sign - erect</td>
      <td>05/06/2015</td>
      <td>0326</td>
      <td>023</td>
      <td>140</td>
      <td>NaN</td>
      <td>Ellis</td>
      <td>St</td>
      <td>...</td>
      <td>3.0</td>
      <td>constr type 3</td>
      <td>NaN</td>
      <td>wood frame (5)</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>Tenderloin</td>
      <td>94102.0</td>
      <td>(37.785719256680785, -122.40852313194863)</td>
      <td>1380611233945</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201604195146</td>
      <td>4</td>
      <td>sign - erect</td>
      <td>04/19/2016</td>
      <td>0306</td>
      <td>007</td>
      <td>440</td>
      <td>NaN</td>
      <td>Geary</td>
      <td>St</td>
      <td>...</td>
      <td>3.0</td>
      <td>constr type 3</td>
      <td>NaN</td>
      <td>wood frame (5)</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>Tenderloin</td>
      <td>94102.0</td>
      <td>(37.78733980600732, -122.41063199757738)</td>
      <td>1420164406718</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201605278609</td>
      <td>3</td>
      <td>additions alterations or repairs</td>
      <td>05/27/2016</td>
      <td>0595</td>
      <td>203</td>
      <td>1647</td>
      <td>NaN</td>
      <td>Pacific</td>
      <td>Av</td>
      <td>...</td>
      <td>1.0</td>
      <td>constr type 1</td>
      <td>1.0</td>
      <td>constr type 1</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>Russian Hill</td>
      <td>94109.0</td>
      <td>(37.7946573324287, -122.42232562979227)</td>
      <td>1424856504716</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201611072166</td>
      <td>8</td>
      <td>otc alterations permit</td>
      <td>11/07/2016</td>
      <td>0156</td>
      <td>011</td>
      <td>1230</td>
      <td>NaN</td>
      <td>Pacific</td>
      <td>Av</td>
      <td>...</td>
      <td>5.0</td>
      <td>wood frame (5)</td>
      <td>5.0</td>
      <td>wood frame (5)</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>Nob Hill</td>
      <td>94109.0</td>
      <td>(37.79595867909168, -122.41557405519474)</td>
      <td>1443574295566</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201611283529</td>
      <td>6</td>
      <td>demolitions</td>
      <td>11/28/2016</td>
      <td>0342</td>
      <td>001</td>
      <td>950</td>
      <td>NaN</td>
      <td>Market</td>
      <td>St</td>
      <td>...</td>
      <td>3.0</td>
      <td>constr type 3</td>
      <td>NaN</td>
      <td>wood frame (5)</td>
      <td>NaN</td>
      <td>6.0</td>
      <td>Tenderloin</td>
      <td>94102.0</td>
      <td>(37.78315261897309, -122.40950883997789)</td>
      <td>144548169992</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 43 columns</p>
</div>



Then some of NAs in the proposed Construction Type Description is successfully change to wood frame(5) which is the most frequent value in the column.

### Convert String Into Float/Integer

---

When we are doing numeric analysis(for example, get the mean value), it is possible that the system will return an error saying that you can't apply the mathematical command to strings. So we want to make sure all numbers are "int" format number instead of string so that it is ready for further analysis.

Covert String into numbers in excel can be really messy. However, in python it is really simple to acheive it. Here we introcude two simple command: float and int.


```python
test_row = mydata.iloc[0:5]
test_row['Permit Type'].astype(float)
```
```



    0    4.0
    1    4.0
    2    3.0
    3    8.0
    4    6.0
```



```python
test_row = mydata.iloc[0:5]
test_row['Permit Type'].astype(int)
```



```
    0    4
    1    4
    2    3
    3    8
    4    6
```



Float command is able to convert the number appear in scientific nutation while int is able to handle non-base int values. So be flexible with two commands


### Remove Unnecessary Duplicates

---

Some times rows with same content will appear twice in your dataframe and you definitely don't want it. How could we handle the unnecessary duplicates?


```python
# create a sample df with 2 rows and fewer columns (for demonstration)
test_df = mydata[['Permit Number', 'Permit Type', 'Block']].iloc[0:2]
# create a df with double info
dup_df = pd.concat([test_df, test_df])
dup_df.head()

# just to demonstrate, see which rows have duplicates
dup_df['Duplicate'] = dup_df.duplicated()
dup_df

# or if you have a unique identifier (like permit number) you can also use it to drop duplicates
# dup_df.drop_duplicates
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Permit Number</th>
      <th>Permit Type</th>
      <th>Block</th>
      <th>Duplicate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201505065519</td>
      <td>4</td>
      <td>0326</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201604195146</td>
      <td>4</td>
      <td>0306</td>
      <td>False</td>
    </tr>
    <tr>
      <th>0</th>
      <td>201505065519</td>
      <td>4</td>
      <td>0326</td>
      <td>True</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201604195146</td>
      <td>4</td>
      <td>0306</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div>



Here we have the duplicated rows and we want get rid of the excess rows. We use drop duplicate command to achieve it.


```python
test_df.drop_duplicates()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Permit Number</th>
      <th>Permit Type</th>
      <th>Block</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201505065519</td>
      <td>4</td>
      <td>0326</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201604195146</td>
      <td>4</td>
      <td>0306</td>
    </tr>
  </tbody>
</table>
</div>



### Finding Out Potential Mismatch

---

The error here is not the typical error that we are getting in python. It is more refering as the logical error in the dataset. It is easier to explain with the dataset.


```python
test_df1 = mydata.iloc[0:5]
test_df1.head(1)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Permit Number</th>
      <th>Permit Type</th>
      <th>Permit Type Definition</th>
      <th>Permit Creation Date</th>
      <th>Block</th>
      <th>Lot</th>
      <th>Street Number</th>
      <th>Street Number Suffix</th>
      <th>Street Name</th>
      <th>Street Suffix</th>
      <th>...</th>
      <th>Existing Construction Type</th>
      <th>Existing Construction Type Description</th>
      <th>Proposed Construction Type</th>
      <th>Proposed Construction Type Description</th>
      <th>Site Permit</th>
      <th>Supervisor District</th>
      <th>Neighborhoods - Analysis Boundaries</th>
      <th>Zipcode</th>
      <th>Location</th>
      <th>Record ID</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201505065519</td>
      <td>4</td>
      <td>sign - erect</td>
      <td>05/06/2015</td>
      <td>0326</td>
      <td>023</td>
      <td>140</td>
      <td>0</td>
      <td>Ellis</td>
      <td>St</td>
      <td>...</td>
      <td>3.0</td>
      <td>constr type 3</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>3.0</td>
      <td>Tenderloin</td>
      <td>94102.0</td>
      <td>(37.785719256680785, -122.40852313194863)</td>
      <td>1380611233945</td>
    </tr>
  </tbody>
</table>
<p>1 rows × 43 columns</p>
</div>



Then what can be a potential mismatch in the dataframe?

Within the dataframe we can see one column called the "Existing Construction Type" and another one called "Existing Constuction Type Description". If we are have a type value of 3 but the the description is constr type 4, then there's definitely some problems happens here.

For a Existing Construction Type of 3, the Existing Construction Type Description would be constr type 3. There is a fixed relationship here. Any rows with different value can be considered as "error" and we need to fix them when we are cleaning the data.

We can Check the mismatch occur or not first:


```python
(mydata.groupby(['Existing Construction Type', 'Existing Construction Type Description'])
    .size()
    .reset_index()
    .rename(columns={0:'count'}))
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Existing Construction Type</th>
      <th>Existing Construction Type Description</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>constr type 1</td>
      <td>28072</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.0</td>
      <td>constr type 2</td>
      <td>4068</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.0</td>
      <td>constr type 3</td>
      <td>9663</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.0</td>
      <td>constr type 4</td>
      <td>381</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>wood frame (5)</td>
      <td>113350</td>
    </tr>
  </tbody>
</table>
</div>



Then let's assume some problems actually happens here:


```python
test_df1["Existing Construction Type"].unique()
print("There are 3 construction Types which are 1, 3 and 5")
print()
print("="*100)
print()

def errorcheck(a):
    if a["Existing Construction Type"] == 3.0:
        a["Existing Construction Type Description"] = "constr type 3"
    if a["Existing Construction Type"] == 1.0:
        a["Existing Construction Type Description"] = "constr type 1"
    if a["Existing Construction Type"] == 5.0:
        a["Existing Construction Type Description"] = "wood frame (5)"
```

    There are 3 construction Types which are 1, 3 and 5
    
    ====================================================================================================
    



```python
for index, row in test_row.iterrows():
    errorcheck(row)
test_row.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Permit Number</th>
      <th>Permit Type</th>
      <th>Permit Type Definition</th>
      <th>Permit Creation Date</th>
      <th>Block</th>
      <th>Lot</th>
      <th>Street Number</th>
      <th>Street Number Suffix</th>
      <th>Street Name</th>
      <th>Street Suffix</th>
      <th>...</th>
      <th>Existing Construction Type</th>
      <th>Existing Construction Type Description</th>
      <th>Proposed Construction Type</th>
      <th>Proposed Construction Type Description</th>
      <th>Site Permit</th>
      <th>Supervisor District</th>
      <th>Neighborhoods - Analysis Boundaries</th>
      <th>Zipcode</th>
      <th>Location</th>
      <th>Record ID</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201505065519</td>
      <td>4</td>
      <td>sign - erect</td>
      <td>05/06/2015</td>
      <td>0326</td>
      <td>023</td>
      <td>140</td>
      <td>0</td>
      <td>ellis</td>
      <td>St</td>
      <td>...</td>
      <td>3.0</td>
      <td>constr type 3</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>3.0</td>
      <td>Tenderloin</td>
      <td>94102.0</td>
      <td>(37.785719256680785, -122.40852313194863)</td>
      <td>1380611233945</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201604195146</td>
      <td>4</td>
      <td>sign - erect</td>
      <td>04/19/2016</td>
      <td>0306</td>
      <td>007</td>
      <td>440</td>
      <td>0</td>
      <td>geary</td>
      <td>St</td>
      <td>...</td>
      <td>3.0</td>
      <td>constr type 3</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>3.0</td>
      <td>Tenderloin</td>
      <td>94102.0</td>
      <td>(37.78733980600732, -122.41063199757738)</td>
      <td>1420164406718</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201605278609</td>
      <td>3</td>
      <td>additions alterations or repairs</td>
      <td>05/27/2016</td>
      <td>0595</td>
      <td>203</td>
      <td>1647</td>
      <td>0</td>
      <td>pacific</td>
      <td>Av</td>
      <td>...</td>
      <td>1.0</td>
      <td>constr type 1</td>
      <td>1.0</td>
      <td>constr type 1</td>
      <td>0</td>
      <td>3.0</td>
      <td>Russian Hill</td>
      <td>94109.0</td>
      <td>(37.7946573324287, -122.42232562979227)</td>
      <td>1424856504716</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201611072166</td>
      <td>8</td>
      <td>otc alterations permit</td>
      <td>11/07/2016</td>
      <td>0156</td>
      <td>011</td>
      <td>1230</td>
      <td>0</td>
      <td>pacific</td>
      <td>Av</td>
      <td>...</td>
      <td>5.0</td>
      <td>wood frame (5)</td>
      <td>5.0</td>
      <td>wood frame (5)</td>
      <td>0</td>
      <td>3.0</td>
      <td>Nob Hill</td>
      <td>94109.0</td>
      <td>(37.79595867909168, -122.41557405519474)</td>
      <td>1443574295566</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201611283529</td>
      <td>6</td>
      <td>demolitions</td>
      <td>11/28/2016</td>
      <td>0342</td>
      <td>001</td>
      <td>950</td>
      <td>0</td>
      <td>market</td>
      <td>St</td>
      <td>...</td>
      <td>3.0</td>
      <td>constr type 3</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>6.0</td>
      <td>Tenderloin</td>
      <td>94102.0</td>
      <td>(37.78315261897309, -122.40950883997789)</td>
      <td>144548169992</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 43 columns</p>
</div>




```python
#there are alternative methods for doing this cleaning
#or use apply and lambda with a simplified function

# a simpler function 

def constr_type(x):
    if x == 3.0:
        descr = "constr type 3"
    elif x == 1.0:
        descr = "constr type 1"
    elif x == 5.0:
        descr = "wood frame (5)"
    else:
        descr = None
    return(descr)

new_df = test_df1
new_df['New Description'] = new_df['Existing Construction Type'].apply(lambda x: constr_type(x))


# or you can map a dictionary
constr_dict = {1.0: "constr type 1",
              3.0: "constr type 3",
              5.0: "wood frame (5)"}

new_df['New Description2'] = new_df['Existing Construction Type'].map(constr_dict)
new_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Permit Number</th>
      <th>Permit Type</th>
      <th>Permit Type Definition</th>
      <th>Permit Creation Date</th>
      <th>Block</th>
      <th>Lot</th>
      <th>Street Number</th>
      <th>Street Number Suffix</th>
      <th>Street Name</th>
      <th>Street Suffix</th>
      <th>...</th>
      <th>Proposed Construction Type</th>
      <th>Proposed Construction Type Description</th>
      <th>Site Permit</th>
      <th>Supervisor District</th>
      <th>Neighborhoods - Analysis Boundaries</th>
      <th>Zipcode</th>
      <th>Location</th>
      <th>Record ID</th>
      <th>New Description</th>
      <th>New Description2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201505065519</td>
      <td>4</td>
      <td>sign - erect</td>
      <td>05/06/2015</td>
      <td>0326</td>
      <td>023</td>
      <td>140</td>
      <td>NaN</td>
      <td>Ellis</td>
      <td>St</td>
      <td>...</td>
      <td>NaN</td>
      <td>wood frame (5)</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>Tenderloin</td>
      <td>94102.0</td>
      <td>(37.785719256680785, -122.40852313194863)</td>
      <td>1380611233945</td>
      <td>constr type 3</td>
      <td>constr type 3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201604195146</td>
      <td>4</td>
      <td>sign - erect</td>
      <td>04/19/2016</td>
      <td>0306</td>
      <td>007</td>
      <td>440</td>
      <td>NaN</td>
      <td>Geary</td>
      <td>St</td>
      <td>...</td>
      <td>NaN</td>
      <td>wood frame (5)</td>
      <td>NaN</td>
      <td>3.0</td>
      <td>Tenderloin</td>
      <td>94102.0</td>
      <td>(37.78733980600732, -122.41063199757738)</td>
      <td>1420164406718</td>
      <td>constr type 3</td>
      <td>constr type 3</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 45 columns</p>
</div>



### Upper/Lower Cases

---

This might be irrelavant to the construction data set. However, there are many reasons that why Upper or Lower case should be considered in data cleaning. There might be human errors while inputing the error and some of the NLP algorithm recognized capitalized data as a facter to express strong opinion. So you want to make sure you are cautious with the case selection.

One of the universal way to remove the case influence is to change everything to upper case first, then lower case again. By doing this, all the cases will be removed.

For example, if we want to remove the Capital letter at the beginning of the street names:


```python
test_row = mydata.iloc[0:5]
test_row["Street Name"] = test_row["Street Name"].str.upper();print(test_row["Street Name"] )
```
```
    0      ELLIS
    1      GEARY
    2    PACIFIC
    3    PACIFIC
    4     MARKET
```



```python
test_row["Street Name"] = test_row["Street Name"].str.lower();print(test_row["Street Name"] )
```
```
    0      ellis
    1      geary
    2    pacific
    3    pacific
    4     market
```


Then there is no upper case left in the dataframe. The NLP algorithm can be applied perfectly to the dataset.


## Next Step

---

Can you tell me which neighborhood has the most type 3 contruction project? If not, what more do we need to do?


## Reference

---

https://www.digitalvidya.com/blog/data-cleaning-techniques/ 


https://www.kaggle.com/rtatman/data-cleaning-challenge-handling-missing-values


https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/


